{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c5d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Sequence, Annotated,List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9a0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7054929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- State Definition ----\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    clarity_status: str\n",
    "    clarification_reason: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    clarification: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    answer: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00e82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321edbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ---- Model Binding ----\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\", temperature=0.3\n",
    ").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdea566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clear(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "        determines if the question is clear. \n",
    "        clear ? answers : clarify()\n",
    "    \"\"\"\n",
    "    question_to_check = state[\"question\"]\n",
    "    if DEBUG:\n",
    "        print(f\"DEBUG is_clear: Checking question: '{question_to_check}'\")\n",
    "    \n",
    "    sys_prompt = SystemMessage(content=\"You are a helpful assistant that determines if a question is clear. Your primary goal is to understand if you have enough information to provide a comprehensive answer directly. If the question is ambiguous, too broad, or seems to be missing crucial context that a user would typically provide, mark it as unclear. Do not be overly critical for simple questions.\")\n",
    "    messages = [\n",
    "        sys_prompt,\n",
    "        HumanMessage(content=f\"Is this question clear enough to answer directly? Respond only with 'clear' or 'unclear: <reason why it's unclear and what clarification is needed>'.\\n\\nQuestion: {question_to_check}\")\n",
    "    ]\n",
    "    llm_response = llm.invoke(messages)\n",
    "    result = llm_response.content.strip()\n",
    "    if DEBUG:\n",
    "        print(f\"DEBUG is_clear: LLM raw response for clarity: '{result}'\")\n",
    "\n",
    "    if result.lower().startswith(\"unclear\"): # More robust check\n",
    "        try:\n",
    "            reason = result.split(\":\", 1)[1].strip()\n",
    "        except IndexError:\n",
    "            reason = \"The LLM indicated the question was unclear but did not provide a specific reason.\"\n",
    "        return {\"clarity_status\": \"unclear\", \"clarification_reason\": [AIMessage(content=reason)]}\n",
    "    else:\n",
    "        # If clear, ensure clarification_reason is empty or not updated to clear old reasons\n",
    "        return {\"clarity_status\": \"clear\", \"clarification_reason\": []}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2a73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(state: AgentState) -> AgentState:\n",
    "    current_question = state[\"question\"]\n",
    "    clarification_reasons_messages = state.get(\"clarification_reason\", [])\n",
    "    user_clarifications_messages = state.get(\"clarification\", [])\n",
    "\n",
    "    context_parts = [\"Please answer the following question:\", current_question]\n",
    "    \n",
    "    if clarification_reasons_messages:\n",
    "        context_parts.append(\"\\nContext - Initial Unclarity & AI Reasoning:\")\n",
    "        for msg in clarification_reasons_messages:\n",
    "            context_parts.append(f\"- AI thought: {msg.content}\")\n",
    "            \n",
    "    if user_clarifications_messages:\n",
    "        context_parts.append(\"\\nContext - User Clarifications:\")\n",
    "        for msg in user_clarifications_messages:\n",
    "            context_parts.append(f\"- User clarified: {msg.content}\")\n",
    "\n",
    "    prompt_content = \"\\n\".join(context_parts)\n",
    "    if DEBUG:\n",
    "        print(f\"DEBUG answer: Prompt content for answer: '{prompt_content}'\")\n",
    "\n",
    "\n",
    "    messages_for_answer = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that answers questions clearly. Use any provided context about previous unclarity and user clarifications to inform your answer to the current question.\"),\n",
    "        HumanMessage(content=prompt_content)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages_for_answer).content.strip()\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e66cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarify(state: AgentState) -> AgentState:\n",
    "    reason_messages = state.get(\"clarification_reason\", [])\n",
    "    reason_for_clarification = \"The question was deemed unclear.\" # Default\n",
    "    if reason_messages and hasattr(reason_messages[-1], 'content'):\n",
    "        reason_for_clarification = reason_messages[-1].content # Get the last reason\n",
    "        \n",
    "    original_question = state[\"question\"]\n",
    "\n",
    "    print(f\"ü§ñ AI: I'm a bit unsure about your question: \\\"{original_question}\\\"\")\n",
    "    print(f\"ü§ñ AI Reason: {reason_for_clarification}\")\n",
    "    print(\"ü§ñ AI: Could you rephrase or provide more details?\")\n",
    "    clarified_input = input(\"üßë You: \")\n",
    "\n",
    "    revised_question = f\"{original_question} (User Clarification: {clarified_input})\"\n",
    "    if DEBUG:\n",
    "        print(f\"DEBUG clarify: Revised question: '{revised_question}'\")\n",
    "    return {\n",
    "        \"question\": revised_question,\n",
    "        \"clarification\": [HumanMessage(content=clarified_input)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29bf860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Graph Definition ----\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"is_clear_node\", is_clear)\n",
    "builder.add_node(\"clarify_node\", clarify)\n",
    "builder.add_node(\"answer_node\", answer)\n",
    "\n",
    "builder.set_entry_point(\"is_clear_node\")\n",
    "builder.add_conditional_edges(\n",
    "    \"is_clear_node\",\n",
    "    lambda x: x[\"clarity_status\"],\n",
    "    {\"clear\": \"answer_node\", \"unclear\": \"clarify_node\"}\n",
    ")\n",
    "builder.add_edge(\"clarify_node\", \"is_clear_node\")\n",
    "builder.add_edge(\"answer_node\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f0a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save Graph Image (Only if DEBUG is True) ----\n",
    "if DEBUG:\n",
    "    try:\n",
    "        with open(\"graph.png\", \"wb\") as f:\n",
    "            f.write(graph.get_graph().draw_mermaid_png())\n",
    "        print(\"‚úÖ Graph image saved as 'graph.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not save graph image: {e}. Pygraphviz and Graphviz might be required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3be8f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Running the graph with DETAILED FLOW ----\n",
    "initial_question = input(\"üßë Ask a question: \")\n",
    "initial_state = AgentState(\n",
    "    question=initial_question,\n",
    "    clarity_status=\"\",\n",
    "    clarification_reason=[], # CRITICAL: Initialize add_messages fields as empty lists\n",
    "    clarification=[],      # CRITICAL: Initialize add_messages fields as empty lists\n",
    "    answer=None\n",
    "    # clarification_attempts=0 # if using\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e1715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your question...\n",
      "\n",
      "üí° AI Answer: 1 + 2 = 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_state_data = None\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\nüöÄ Starting Graph Execution Flow (DEBUG MODE)...\\n\")\n",
    "    cumulative_state_for_display = initial_state.copy() # For display purposes in debug mode\n",
    "\n",
    "    for i, event_chunk in enumerate(graph.stream(initial_state, {\"recursion_limit\": 10})):\n",
    "        for node_name, node_output_dict in event_chunk.items():\n",
    "            print(f\"--- Step {i+1}: Node Executed: '{node_name}' ---\")\n",
    "            print(\"  Node Output:\")\n",
    "            if not node_output_dict:\n",
    "                print(\"    (No direct output values returned or node is END)\")\n",
    "            for key, value in node_output_dict.items():\n",
    "                if isinstance(value, list) and all(isinstance(item, BaseMessage) for item in value):\n",
    "                    print(f\"    {key}:\")\n",
    "                    for msg_idx, msg in enumerate(value):\n",
    "                        print(f\"      [{msg_idx}] {type(msg).__name__}(content=\\\"{msg.content}\\\")\")\n",
    "                else:\n",
    "                    print(f\"    {key}: {value}\")\n",
    "\n",
    "            # Update display copy of the state (LangGraph handles true state)\n",
    "            for key, value in node_output_dict.items():\n",
    "                if key in cumulative_state_for_display:\n",
    "                    if isinstance(cumulative_state_for_display.get(key), list) and \\\n",
    "                       isinstance(value, list) and \\\n",
    "                       key in AgentState.__annotations__ and \\\n",
    "                       \"add_messages\" in str(AgentState.__annotations__[key]):\n",
    "                        cumulative_state_for_display[key].extend(value)\n",
    "                    else:\n",
    "                        cumulative_state_for_display[key] = value\n",
    "            \n",
    "            if node_name != END: # Don't print full state for END node as it might be empty\n",
    "                print(\"  Current Relevant State (after this node's contribution):\")\n",
    "                print(f\"    Question: \\\"{cumulative_state_for_display['question']}\\\"\")\n",
    "                print(f\"    Clarity Status: \\\"{cumulative_state_for_display['clarity_status']}\\\"\")\n",
    "\n",
    "                if cumulative_state_for_display['clarification_reason']:\n",
    "                    print(\"    Accumulated Clarification Reasons (AI):\")\n",
    "                    for msg_idx, msg in enumerate(cumulative_state_for_display['clarification_reason']):\n",
    "                        print(f\"      [{msg_idx}] {type(msg).__name__}(content=\\\"{msg.content}\\\")\")\n",
    "                else:\n",
    "                    print(\"    Accumulated Clarification Reasons (AI): []\")\n",
    "\n",
    "                if cumulative_state_for_display['clarification']:\n",
    "                    print(\"    Accumulated Clarifications (User):\")\n",
    "                    for msg_idx, msg in enumerate(cumulative_state_for_display['clarification']):\n",
    "                        print(f\"      [{msg_idx}] {type(msg).__name__}(content=\\\"{msg.content}\\\")\")\n",
    "                else:\n",
    "                    print(\"    Accumulated Clarifications (User): []\")\n",
    "                \n",
    "                if cumulative_state_for_display['answer']:\n",
    "                    print(f\"    Answer: \\\"{cumulative_state_for_display['answer']}\\\"\")\n",
    "\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            if node_name == END:\n",
    "                print(\"\\nüèÅ Graph has reached END.\")\n",
    "                final_state_data = cumulative_state_for_display # Use the state we tracked\n",
    "                break\n",
    "        if final_state_data and node_name == END: # break outer loop if inner loop was broken by END\n",
    "            break\n",
    "else: # Normal operation (DEBUG is False)\n",
    "    print(\"Processing your question...\") # Minimal feedback\n",
    "    final_state_data = graph.invoke(initial_state, {\"recursion_limit\": 10})\n",
    "\n",
    "# ---- Final Output ----\n",
    "if final_state_data and final_state_data.get(\"answer\"):\n",
    "    print(\"\\nüí° AI Answer:\", final_state_data[\"answer\"])\n",
    "elif final_state_data:\n",
    "    print(\"\\nü§î AI: I couldn't arrive at a final answer for your question after the process.\")\n",
    "    if DEBUG and final_state_data.get('clarification_reason'):\n",
    "        print(\"    Last reason for unclarity noted by AI:\")\n",
    "        for msg in final_state_data['clarification_reason']:\n",
    "             print(f\"      - {type(msg).__name__}(content=\\\"{msg.content}\\\")\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è An unexpected error occurred, and no final state was determined.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
